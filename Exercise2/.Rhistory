ggplot(data = Auto,  aes(y = Auto$mpg)) +
geom_boxplot(y)
getwd
getwd()
ggplot(data = Auto,  aes(y = Auto$mpg, x = Auto$cylinders)) +
geom_boxplot()
str(Auto)
ggplot(data = Auto,  aes(y = Auto$mpg, x = Auto$origin)) +
geom_boxplot()
?aes
ggplot(data = Auto,  aes(as.factor(cylinders), mpg)) +
geom_boxplot(fill="skyblue") +
labs(title = "mpg vs cylinders")
ggplot(data = Auto,  aes(as.factor(origin), mpg)) +
geom_boxplot(fill="skyblue") +
labs(title = "mpg vs origin")
?Auto
reduced_auto_quant = data.frame(reduced_auto$mpg,
reduced_auto$displacement,
reduced_auto$horsepower,
reduced_auto$weight,
reduced_auto$acceleration,
reduced_auto$year)
reduced_ranges = sapply(reduced_auto_quant, range, simplify = TRUE)
reduced_means = sapply(reduced_auto_quant, mean, simplify = TRUE)
reduced_std_dev = sapply(reduced_auto_quant, sd, simplify = TRUE)
quant = data.frame(Auto$mpg,
Auto$displacement,
Auto$horsepower,
Auto$weight,
Auto$acceleration,
Auto$year)
ranges = sapply(quant, range, simplify = TRUE)
means = sapply(quant, mean, simplify = TRUE)
std_dev = sapply(quant, sd, simplify = TRUE)
ggplot(data = Auto,  aes(as.factor(origin), mpg)) +
geom_boxplot(fill="skyblue") +
labs(title = "mpg vs origin") +
legend(x=0,y=50, c("1=American","2=European","3=Japanese"))
ggplot(data = Auto,  aes(as.factor(origin), mpg)) +
geom_boxplot(fill="skyblue") +
labs(title = "mpg vs origin")
quant = c(1,3,4,5,6,7)
covMat = cov(Auto[,quant])
View(covMat)
sd_products = std_dev %*% t(std_dev)
View(sd_products)
sd_products = as.numeric(std_dev) %*% t(as.numeric(std_dev))
corMat = covMat / sd_products
corMat2 = cor(Auto[,quant])
View(corMat)
View(corMat2)
?matrix
mu1 = c(2,3)
sigma1 = matrix(c(1,0,0,1),nrow=2,ncol=2)
mu = c(2,3)
library(MASS)
mu = c(2,3)
sigma1 = matrix(c(1,0,0,1),nrow=2,ncol=2)
sigma2 = matrix(c(1,0,0,5),nrow=2,ncol=2)
sigma3 = matrix(c(1,2,2,5),nrow=2,ncol=2)
sigma4 = matrix(c(1,-2,-2,5),nrow=2,ncol=2)
mvnorm?
?mvnorm
?mvnorm()
mvnorm
mvnorm()
MASS
?mvrnorm
simulated_data = data.frame(mvnorm(n,mu,sigma1),
mvrnorm(n,mu,sigma2),
mvrnorm(n,mu,sigma3),
mvrnorm(n,mu,sigma4))
simulated_data = data.frame(mvrnorm(n,mu,sigma1),
mvrnorm(n,mu,sigma2),
mvrnorm(n,mu,sigma3),
mvrnorm(n,mu,sigma4))
n = 1000
mu = c(2,3)
sigma1 = matrix(c(1,0,0,1),nrow=2,ncol=2)
sigma2 = matrix(c(1,0,0,5),nrow=2,ncol=2)
sigma3 = matrix(c(1,2,2,5),nrow=2,ncol=2)
sigma4 = matrix(c(1,-2,-2,5),nrow=2,ncol=2)
simulated_data = data.frame(mvrnorm(n,mu,sigma1),
mvrnorm(n,mu,sigma2),
mvrnorm(n,mu,sigma3),
mvrnorm(n,mu,sigma4))
variables = c("x1","x2")
set1 = as.data.frame(mvrnorm(n,mu,sigma1))
set1 = as.data.frame(mvrnorm(n,mu,sigma1))
colnames(set1) = variables
set2 = as.data.frame(mvrnorm(n,mu,sigma2))
colnames(set2) = variables
set3 = as.data.frame(mvrnorm(n,mu,sigma3))
colnames(set3) = variables
set4 = as.data.frame(mvrnorm(n,mu,sigma4))
colnames(set4) = variables
install.packages("patchwork")
p1 = ggplot(set1, aes(x1,x2)) + geom_point() + labs(title = "set1")
p1 = ggplot(set1, aes(x1,x2)) + geom_point() + labs(title = "set1")
p2 = ggplot(set2, aes(x1,x2)) + geom_point() + labs(title = "set2")
p3 = ggplot(set3, aes(x1,x2)) + geom_point() + labs(title = "set3")
p4 = ggplot(set4, aes(x1,x2)) + geom_point() + labs(title = "set4")
(p1 + p2) / (p3 + p4)
library(patchwork)
p1 = ggplot(set1, aes(x1,x2)) + geom_point() + labs(title = "set1")
p2 = ggplot(set2, aes(x1,x2)) + geom_point() + labs(title = "set2")
p3 = ggplot(set3, aes(x1,x2)) + geom_point() + labs(title = "set3")
p4 = ggplot(set4, aes(x1,x2)) + geom_point() + labs(title = "set4")
(p1 + p2) / (p3 + p4)
p1 = ggplot(set1, aes(x1,x2)) + geom_point() + labs(title = "set1") + xlim(-5,10)
(p1 + p2) / (p3 + p4)
p2 = ggplot(set2, aes(x1,x2)) + geom_point() + labs(title = "set2") + xlim(-5,10) + ylim(-5,10)
p3 = ggplot(set3, aes(x1,x2)) + geom_point() + labs(title = "set3") + xlim(-5,10) + ylim(-5,10)
p4 = ggplot(set4, aes(x1,x2)) + geom_point() + labs(title = "set4") + xlim(-5,10) + ylim(-5,10)
(p1 + p2) / (p3 + p4)
p1 = ggplot(set1, aes(x1,x2)) + geom_point() + labs(title = "set1") + xlim(-5,10) + ylim(-5,10)
(p1 + p2) / (p3 + p4)
set.seed(2) # to reproduce
M <- 100 # repeated samplings, x fixed
nord <- 20 # order of polynomials
x <- seq(from = -2, to = 4, by = 0.1)
truefunc <- function(x) {
return(x ^ 2)
}
true_y <- truefunc(x)
error <- matrix(rnorm(length(x) * M, mean = 0, sd = 2),
nrow = M,
byrow = TRUE)
?rep
ymat <- matrix(rep(true_y, M), byrow = T, nrow = M) + error  # Each row is a simulation
predictions_list <- lapply(1:nord, matrix, data = NA, nrow = M, ncol = ncol(ymat))
for(i in 1:nord){
for(j in 1:M){
predictions_list[[i]][j, ] <- predict(lm(ymat[j,] ~ poly(x, i, raw = TRUE)))
}
}
library(tidyverse) # The tidyverse contains ggplot2, as well as tidyr and dplyr,
list_of_matrices_with_deg_id <-
lapply(1:nord,
function(poly_degree){cbind(predictions_list[[poly_degree]],
simulation_num = 1:M, poly_degree)}
)
# Extract each matrix and bind them to one large matrix
stacked_matrices <-  NULL
for (i in 1:nord) {
stacked_matrices <-
rbind(stacked_matrices, list_of_matrices_with_deg_id[[i]])
}
stacked_matrices_df <- as.data.frame(stacked_matrices)
# Convert from wide to long (because that is the best format for ggplot2)
long_predictions_df <- pivot_longer(stacked_matrices_df,
!c(simulation_num, poly_degree),
values_to = "y")
plotting_df <- cbind(long_predictions_df, x = x) %>% # adding the x-vector to the dataframe
filter(poly_degree %in% c(1, 2, 10, 20)) # Select only the predictions using degree 1, 2, 10 or 20
ggplot(plotting_df, aes(x = x, y = y, group = simulation_num)) +
geom_line(aes(color = simulation_num)) +
geom_line(aes(x = x, y = x^2), size = 1.5) +
facet_wrap(~ poly_degree) +
theme_bw() +
theme(legend.position = "none")
?lapply
#------
# Create list of empty matrices
predictions_list <- lapply(1:nord, matrix, data = NA, nrow = M, ncol = ncol(ymat))
View(predictions_list)
# Iterate through each matrix in the list
for(j in 1:M){
# iterate through each row of current matrix, fit polynomial of degree i to simulated data (ymat[j,])
predictions_list[[i]][j, ] <- predict(lm(ymat[j,] ~ poly(x, i, raw = TRUE)))
}
View(predictions_list)
set.seed(2) # to reproduce
M <- 100 # repeated samplings, x fixed
nord <- 20 # order of polynomials
x <- seq(from = -2, to = 4, by = 0.1)
truefunc <- function(x) {
return(x ^ 2)
}
# vector with values of f(x)=x^2
true_y <- truefunc(x)
# Produce x amount of samples from N(0,2^2) distribution M times, ends up as a x*M x 1 vector, reshape
# into matrix with M rows so we get x random numbers for M simulations, each row is a simulation
error <- matrix(rnorm(length(x) * M, mean = 0, sd = 2),
nrow = M,
byrow = TRUE)
# Produce matrix M with M rows and length(x) columns, where each row is its own simulation
# of f(x) = x^2 + error. replicate true_y M times, insert as rows, add error element-wise
ymat <- matrix(rep(true_y, M), byrow = T, nrow = M) + error  # Each row is a simulation
#------
# Create list of empty matrices with same dimensions as ymat
predictions_list <- lapply(1:nord, matrix, data = NA, nrow = M, ncol = ncol(ymat))
for(i in 1:nord){
# Iterate through each matrix in the list
for(j in 1:M){
# iterate through each row of current matrix, fit polynomial of degree i to simulated data (ymat[j,])
predictions_list[[i]][j, ] <- predict(lm(ymat[j,] ~ poly(x, i, raw = TRUE)))
}
}
View(predictions_list)
?predict
predictions_list[1]
predictions_list[[1]]
#------
# Create list of empty matrices with same dimensions as ymat
predictions_list <- lapply(1:nord, matrix, data = NA, nrow = M, ncol = ncol(ymat))
for(i in 1:nord){
# Iterate through each matrix in the list
for(j in 1:M){
# iterate through each row of current matrix, fit polynomial of degree i to simulated data (ymat[j,])
predictions_list[i][j, ] <- predict(lm(ymat[j,] ~ poly(x, i, raw = TRUE)))
}
}
# iterate through each row of current matrix, fit polynomial of degree i to simulated data (ymat[j,])
predictions_list[[i]][j, ] <- predict(lm(ymat[j,] ~ poly(x, i, raw = TRUE)))
#------
# Create list of empty matrices with same dimensions as ymat
predictions_list <- lapply(1:nord, matrix, data = NA, nrow = M, ncol = ncol(ymat))
for(i in 1:nord){
# Iterate through each matrix in the list
for(j in 1:M){
# iterate through each row of current matrix, fit polynomial of degree i to simulated data (ymat[j,])
predictions_list[[i]][j, ] <- predict(lm(ymat[j,] ~ poly(x, i, raw = TRUE)))
}
}
?poly
?cbind
library(tidyverse) # The tidyverse contains ggplot2, as well as tidyr and dplyr,
# Apply new function to 1:nord (polynomial degree), bind each matrix with new columns
# indicating simulation number and polynomial degree
list_of_matrices_with_deg_id <-
lapply(1:nord,
function(poly_degree){cbind(predictions_list[[poly_degree]],
simulation_num = 1:M, poly_degree)}
)
View(predictions_list)
View(list_of_matrices_with_deg_id)
str(list_of_matrices_with_deg_id)
list_of_matrices_with_deg_id[[7]]
# Extract each matrix and bind them to one large matrix
stacked_matrices <-  NULL
for (i in 1:nord) {
stacked_matrices <-
rbind(stacked_matrices, list_of_matrices_with_deg_id[[i]])
}
View(stacked_matrices)
stacked_matrices_df <- as.data.frame(stacked_matrices)
View(stacked_matrices_df)
?pivot_longer
?!c
long_predictions_df <- pivot_longer(stacked_matrices_df,
!c(simulation_num, poly_degree),
values_to = "y")
View(long_predictions_df)
plotting_df <- cbind(long_predictions_df, x = x) %>% # adding the x-vector to the dataframe
filter(poly_degree %in% c(1, 2, 10, 20)) # Select only the predictions using degree 1, 2, 10 or 20
?%>%
# Now we can use ggplot2!
# We just want to plot for degrees 1, 2, 10 and 20.
# %>% passes the LHS to the first argument of the RHS, we're able to cbind the length-61 column vector x to
# the length-122000 columns of long_predictions_df since R wraps stuff to the end... x = x gives the column
# the name x
plotting_df <- cbind(long_predictions_df, x = x) %>% # adding the x-vector to the dataframe
filter(poly_degree %in% c(1, 2, 10, 20)) # Select only the predictions using degree 1, 2, 10 or 20
?ggplot
?aes
ggplot(plotting_df, aes(x = x, y = y, group = simulation_num)) +
geom_line(aes(color = simulation_num)) +
geom_line(aes(x = x, y = x^2), size = 1.5) +
facet_wrap(~ poly_degree) +
#theme_bw() +
theme(legend.position = "none")
ggplot(plotting_df, aes(x = x, y = y, group = simulation_num)) +
geom_line(aes(color = simulation_num)) +
geom_line(aes(x = x, y = x^2), size = 1.5) +
#facet_wrap(~ poly_degree) +
#theme_bw() +
theme(legend.position = "none")
ggplot(plotting_df, aes(x = x, y = y, group = simulation_num)) +
geom_line(aes(color = simulation_num)) +
geom_line(aes(x = x, y = x^2), size = 1.5) +
facet_wrap(~ poly_degree) +
#theme_bw() +
theme(legend.position = "none")
# Now we can use ggplot2!
# We just want to plot for degrees 1, 2, 10 and 20.
# %>% passes the LHS to the first argument of the RHS, we're able to cbind the length-61 column vector x to
# the length-122000 columns of long_predictions_df since R wraps stuff to the end... x = x gives the column
# the name x. We only want the rows containing poly_degree 1,2,10, or 20, filter() does this for us
plotting_df <- cbind(long_predictions_df, x = x) %>% # adding the x-vector to the dataframe
filter(poly_degree %in% c(1, 2, 4, 8, 10, 15, 20)) # Select only the predictions using degree 1, 2, 10 or 20
ggplot(plotting_df, aes(x = x, y = y, group = simulation_num)) +
geom_line(aes(color = simulation_num)) +
geom_line(aes(x = x, y = x^2), size = 1.5) +
facet_wrap(~ poly_degree) +
#theme_bw() +
theme(legend.position = "none")
# Now we can use ggplot2!
# We just want to plot for degrees 1, 2, 10 and 20.
# %>% passes the LHS to the first argument of the RHS, we're able to cbind the length-61 column vector x to
# the length-122000 columns of long_predictions_df since R wraps stuff to the end... x = x gives the column
# the name x. We only want the rows containing poly_degree 1,2,10, or 20, filter() does this for us
plotting_df <- cbind(long_predictions_df, x = x) %>% # adding the x-vector to the dataframe
filter(poly_degree %in% c(1, 2, 3,4, 8, 10,12, 15, 20)) # Select only the predictions using degree 1, 2, 10 or 20
ggplot(plotting_df, aes(x = x, y = y, group = simulation_num)) +
geom_line(aes(color = simulation_num)) +
geom_line(aes(x = x, y = x^2), size = 1.5) +
facet_wrap(~ poly_degree) +
#theme_bw() +
theme(legend.position = "none")
# Now we can use ggplot2!
# We just want to plot for degrees 1, 2, 10 and 20.
# %>% passes the LHS to the first argument of the RHS, we're able to cbind the length-61 column vector x to
# the length-122000 columns of long_predictions_df since R wraps stuff to the end... x = x gives the column
# the name x. We only want the rows containing poly_degree 1,2,10, or 20, filter() does this for us
plotting_df <- cbind(long_predictions_df, x = x) %>% # adding the x-vector to the dataframe
filter(poly_degree %in% c(1, 2, 10, 20)) # Select only the predictions using degree 1, 2, 10 or 20
ggplot(plotting_df, aes(x = x, y = y, group = simulation_num)) +
geom_line(aes(color = simulation_num)) +
geom_line(aes(x = x, y = x^2), size = 1.5) +
facet_wrap(~ poly_degree) +
#theme_bw() +
theme(legend.position = "none")
set.seed(2) # to reproduce
M <- 100 # repeated samplings,x fixed but new errors
nord <- 20
x <- seq(from = -2, to = 4, by = 0.1)
truefunc <- function(x){
return(x^2)
}
true_y <- truefunc(x)
error <- matrix(rnorm(length(x)*M, mean = 0, sd = 2), nrow = M, byrow = TRUE)
testerror <- matrix(rnorm(length(x)*M, mean = 0, sd = 2), nrow = M, byrow = TRUE)
ymat <- matrix(rep(true_y, M), byrow = T, nrow = M) + error
testymat <- matrix(rep(true_y, M), byrow=T, nrow=M) + testerror
# Model is fitted to ymat (training data)
predictions_list <- lapply(1:nord, matrix, data = NA, nrow = M, ncol = ncol(ymat))
for(i in 1:nord){
for(j in 1:M){
# Train model, make prediction
predictions_list[[i]][j, ] <- predict(lm(ymat[j,] ~ poly(x, i, raw = TRUE)))
}
}
# Calculate MSE for training data
trainMSE <- lapply(1:nord,
function(poly_degree){
rowMeans((predictions_list[[poly_degree]] - ymat)^2)}
)
# Calculate MSE for test data
testMSE <- lapply(1:nord,
function(poly_degree){
rowMeans((predictions_list[[poly_degree]] - testymat)^2)}
)
# Chunk 1: setup
showsolA<-TRUE
showsolB<-TRUE
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
cache=TRUE, size="scriptsize")
# Chunk 2
library(ISLR)
data(Auto)
# Chunk 3
quantitative = data.frame(Auto$mpg,
Auto$displacement,
Auto$horsepower,
Auto$weight,
Auto$acceleration,
Auto$year)
ranges = sapply(quantitative, range, simplify = TRUE)
# Chunk 4
means = sapply(quantitative, mean, simplify = TRUE)
std_dev = sapply(quantitative, sd, simplify = TRUE)
# Chunk 5
exclusion_indeces = c(10:85)
reduced_auto = Auto[-exclusion_indeces,]
reduced_auto_quant = data.frame(reduced_auto$mpg,
reduced_auto$displacement,
reduced_auto$horsepower,
reduced_auto$weight,
reduced_auto$acceleration,
reduced_auto$year)
reduced_ranges = sapply(reduced_auto_quant, range, simplify = TRUE)
reduced_means = sapply(reduced_auto_quant, mean, simplify = TRUE)
reduced_std_dev = sapply(reduced_auto_quant, sd, simplify = TRUE)
# Chunk 6
library(GGally)
ggpairs(quantitative)
# Chunk 7
ggplot(data = Auto,  aes(as.factor(cylinders), mpg)) +
geom_boxplot(fill="skyblue") +
labs(title = "mpg vs cylinders")
ggplot(data = Auto,  aes(as.factor(origin), mpg)) +
geom_boxplot(fill="skyblue") +
labs(title = "mpg vs origin")
# Chunk 8
quant = c(1,3,4,5,6,7)
covMat = cov(Auto[,quant])
sd_products = as.numeric(std_dev) %*% t(as.numeric(std_dev))
corMat = covMat / sd_products
corMat2 = cor(Auto[,quant])
# Chunk 9
library(MASS)
n = 1000
variables = c("x1","x2")
mu = c(2,3)
sigma1 = matrix(c(1,0,0,1),nrow=2,ncol=2)
sigma2 = matrix(c(1,0,0,5),nrow=2,ncol=2)
sigma3 = matrix(c(1,2,2,5),nrow=2,ncol=2)
sigma4 = matrix(c(1,-2,-2,5),nrow=2,ncol=2)
set1 = as.data.frame(mvrnorm(n,mu,sigma1))
colnames(set1) = variables
set2 = as.data.frame(mvrnorm(n,mu,sigma2))
colnames(set2) = variables
set3 = as.data.frame(mvrnorm(n,mu,sigma3))
colnames(set3) = variables
set4 = as.data.frame(mvrnorm(n,mu,sigma4))
colnames(set4) = variables
# Chunk 10
library(patchwork)
p1 = ggplot(set1, aes(x1,x2)) + geom_point() + labs(title = "set1") + xlim(-5,10) + ylim(-5,10)
p2 = ggplot(set2, aes(x1,x2)) + geom_point() + labs(title = "set2") + xlim(-5,10) + ylim(-5,10)
p3 = ggplot(set3, aes(x1,x2)) + geom_point() + labs(title = "set3") + xlim(-5,10) + ylim(-5,10)
p4 = ggplot(set4, aes(x1,x2)) + geom_point() + labs(title = "set4") + xlim(-5,10) + ylim(-5,10)
(p1 + p2) / (p3 + p4)
library(tidyverse) # The tidyverse contains ggplot2, as well as tidyr and dplyr,
# which we can use for dataframe manipulation.
# Convert each matrix in the list form wide to long (because that is the best format for ggplot2)
list_train_MSE <- lapply(1:nord, function(poly_degree) cbind(error = trainMSE[[poly_degree]],
poly_degree,
error_type = "train",
simulation_num = 1:M))
list_test_MSE <- lapply(1:nord, function(poly_degree) cbind(error = testMSE[[poly_degree]],
poly_degree,
error_type = "test",
simulation_num = 1:M))
# Now predictions_list is a list with 20 entries, where each entry is a matrix
# with 100 rows, where each row is the predicted polynomial of that degree.
stacked_train <- NULL
for (i in 1:nord) {
stacked_train <-
rbind(stacked_train, list_train_MSE[[i]])
}
stacked_test <- NULL
for (i in 1:nord) {
stacked_test <-
rbind(stacked_test, list_test_MSE[[i]])
}
stacked_errors_df <- as.data.frame(rbind(stacked_train, stacked_test))
# This is already on long format.
stacked_errors_df$error <- as.numeric(stacked_errors_df$error)
stacked_errors_df$simulation_num <- as.integer(stacked_errors_df$simulation_num)
stacked_errors_df$poly_degree <- as.integer(stacked_errors_df$poly_degree)
p.all_lines <- ggplot(data = stacked_errors_df,
aes(x = poly_degree, y = error, group = simulation_num)) +
geom_line(aes(color = simulation_num)) +
facet_wrap(~ error_type) +
xlab("Polynomial degree") +
ylab("MSE") +
theme_bw() +
theme(legend.position = "none")
p.bars <- ggplot(stacked_errors_df, aes(x = as.factor(poly_degree), y = error)) +
geom_boxplot(aes(fill = error_type)) +
scale_fill_discrete(name = "Error type") +
xlab("Polynomial degree") +
ylab("MSE") +
theme_bw()
# Here we find the average test error and training error across the repeated simulations.
# The symbol "%>%" is called a pipe, and comes from the tidyverse packages,
# which provide convenient functions for working with data frames.
means_across_simulations <- stacked_errors_df %>%
group_by(error_type, poly_degree) %>%
summarise(mean = mean(error))
p.means <- ggplot(means_across_simulations, aes(x = poly_degree, y = mean)) +
geom_line(aes(color = error_type)) +
scale_color_discrete(name = "Error type") +
xlab("Polynomial degree") +
ylab("MSE") +
theme_bw()
library(patchwork) # The library patchwork is the best way of combining ggplot2 objects.
# You could also use the function ggarrange from the ggpubr package.
p.all_lines / (p.bars + p.means)
?apply
meanmat <- matrix(ncol = length(x), nrow = nord)
varmat <- matrix(ncol = length(x), nrow = nord)
for (j in 1:nord){
meanmat[j,] <- apply(predictions_list[[j]], 2, mean) # we now take the mean over the M simulations - to mimic E and Var at each x value and each poly model
varmat[j,] <- apply(predictions_list[[j]], 2, var)
}
# nord times length(x)
bias2mat <- (meanmat - matrix(rep(true_y, nord), byrow = TRUE, nrow = nord))^2 #here the truth is finally used!
# nord times length(x)
bias2mat <- (meanmat - matrix(rep(true_y, nord), byrow = TRUE, nrow = nord))^2 #here the truth is finally used!
Plotting the polys as a function of x:
```{r 2bbiasvariance1, eval=FALSE}
df <- data.frame(x = rep(x, each = nord), poly_degree = rep(1:nord, length(x)),
bias2 = c(bias2mat), variance = c(varmat),
irreducible_error = rep(4, prod(dim(varmat)))) #irr is just 1
df$total <- df$bias2 + df$variance + df$irreducible_error
df_long <- pivot_longer(df, cols = !c(x, poly_degree), names_to = "type")
df_select_poly <- filter(df_long, poly_degree %in% c(1, 2, 10, 20))
ggplot(df_select_poly, aes(x = x, y = value, group = type)) +
geom_line(aes(color = type)) +
facet_wrap(~poly_degree, scales = "free", labeller = label_both) +
theme_bw()
df_select_x <- filter(df_long, x %in% c(-1, 0.5, 2, 3.5))
ggplot(df_select_x, aes(x = poly_degree, y = value, group = type)) +
geom_line(aes(color = type)) +
facet_wrap(~x, scales = "free", labeller = label_both) +
theme_bw()
