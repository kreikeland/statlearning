\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[10pt,ignorenonframetext,]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph:
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
\centering
\begin{beamercolorbox}[sep=16pt,center]{part title}
  \usebeamerfont{part title}\insertpart\par
\end{beamercolorbox}
}
\setbeamertemplate{section page}{
\centering
\begin{beamercolorbox}[sep=12pt,center]{part title}
  \usebeamerfont{section title}\insertsection\par
\end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
\centering
\begin{beamercolorbox}[sep=8pt,center]{part title}
  \usebeamerfont{subsection title}\insertsubsection\par
\end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
\usetheme[]{Singapore}
\usefonttheme{serif}
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Module 9: Support Vector Machines},
            pdfauthor={Stefanie Muff, Department of Mathematical Sciences, NTNU},
            colorlinks=true,
            linkcolor=Maroon,
            filecolor=Maroon,
            citecolor=Blue,
            urlcolor=blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\newif\ifbibliography
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{multirow}

\title{Module 9: Support Vector Machines}
\providecommand{\subtitle}[1]{}
\subtitle{TMA4268 Statistical Learning V2021}
\author{Stefanie Muff, Department of Mathematical Sciences, NTNU}
\date{March 15 and 16, 2021}

\begin{document}
\frame{\titlepage}

\begin{frame}{Acknowledgements}
\protect\hypertarget{acknowledgements}{}

\begin{itemize}
\item
  The course was originally developed by Mette Langaas (original
  material:
  \url{https://github.com/mettelang/StatisticalLearningSpring2019}).
  Mette did a fantastic job and I am very thankful that I was allowed to
  modify and use her material.
\item
  Some of the figures and slides in this presentation are taken (or are
  inspired) from James et al. (2013).
\end{itemize}

\end{frame}

\begin{frame}{Introduction}
\protect\hypertarget{introduction}{}

\begin{block}{Learning material for this module}

\vspace{2mm}

\begin{itemize}
\item
  James et al (2013): An Introduction to Statistical Learning. Chapter
  9.
\item
  All the material presented on these module slides and in class.
\item
  Check also the ``Further reading'' slide at the end of these module
  slides.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{What will you learn?}

\vspace{2mm}

You will get to know

\(~\)

\begin{itemize}
\tightlist
\item
  Maximal margin classifier
\item
  Support vector classifier
\item
  Support vector machines
\item
  Extensions
\item
  Comparisons
\end{itemize}

\(~\)

and learn how to apply all that.

\end{block}

\end{frame}

\begin{frame}{Support Vector Machines}
\protect\hypertarget{support-vector-machines}{}

\(~\)

Approach the two-class classification problem in a direct way:

\centering

\emph{\textbf{We try to find \textcolor{red}{a plane} that separates the
two categories in the feature (covariate) space.}}

\(~\)

\flushleft

If this is not possible, we soften our requirement in two ways:

\begin{itemize}
\item
  We soften what we mean by ``separates''.
\item
  We enrich and enlarge the feature space such that we can do a
  separation
\end{itemize}

\(~\)

Support Vector Machines (SVMs) are a computer science method, not really
a statistical method (no probabilities involved).

\end{frame}

\begin{frame}

\begin{block}{Motivating example}

\vspace{2mm}

\begin{itemize}
\item
  Suppose that you are interested in the distribution of two tree types:
  redwood and pines.
\item
  You have three different study areas in which these trees grow.
\item
  Your study areas with the tree positions in three forests is
  visualized in the figures below.\\
  \textcolor{red}{Redwood tree}; \textcolor{green}{pine tree}.
\end{itemize}

\vspace{2mm}

\begin{center}\includegraphics[width=0.8\linewidth]{9SVM_files/figure-beamer/trees-1} \end{center}

\vspace{0mm}

\begin{itemize}
\tightlist
\item
  You want to build one continuous fence to separate the two tree types
  in each of the three study areas. \textbf{Where should you build the
  fence?}
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

Three cases: \vspace{2mm}

\begin{itemize}
\item
  Forest 1 seems easy: The orange and green points are clearly separated
  and the fence can be built anywhere inside the band that separates
  them. However, there are infinitely many solutions, and we should take
  into account that the trees reproduce and that we want future pines
  and future redwoods to grow up on the correct side of the fence.
\item
  Forest 2 is a bit more complicated: A linear fence still seems like a
  good idea, but in this case the two tree types cannot be perfectly
  separated. You have to allow some errors.
\item
  Forest 3 is the most complex: It is not possible to separate the two
  tree types by a straight line without getting a large number of
  misclassifications. Here, a circular fence around the pine trees seems
  like a reasonable choice.
\end{itemize}

\end{frame}

\begin{frame}

Forest 1 illustrates the problem of finding an \textbf{optimal
separating hyperplane} for a dataset.

Topic: \textbf{Maximal Margin hyperplanes}

\begin{center}\includegraphics[width=0.5\linewidth]{9SVM_files/figure-beamer/forest1-1} \end{center}

\end{frame}

\begin{frame}

You are also going to learn how you can find an optimal separating
hyperplane when your data cannot be perfectly separated by a straight
line.

Topic: \textbf{Support Vector Classifier} or \textbf{Soft Margin
Classifier}

\begin{center}\includegraphics[width=0.5\linewidth]{9SVM_files/figure-beamer/forest2-1} \end{center}

\end{frame}

\begin{frame}

The Support vector classifier can be generalised to an approach that
produces non-linear decision boundaries.

\vspace{2mm}

Topic: \textbf{Support Vector Machines} (SVMs)

\begin{center}\includegraphics[width=0.5\linewidth]{9SVM_files/figure-beamer/forest3-1} \end{center}

\end{frame}

\begin{frame}

Why are we interested in an optimal hyperplane or a separating curve?

\(\rightarrow\) We want to use it for classification.

\begin{itemize}
\item
  In our example: Is it likely that a random seed found at location
  \((0.8,0.4)\) becomes a redwood or a pine tree given the observed
  data?
\item
  Classification of a new observation based on which side of the
  decision boundary it falls into.
\end{itemize}

\vspace{2mm}

\textbf{Note}: In this module we look at \textbf{binary classification},
but extensions to more than two classes are briefly mentioned.

\end{frame}

\begin{frame}

\begin{itemize}
\item
  We will use the three forest examples throughout this module.
\item
  The two covaraites (\(x_1\), \(x_2\)) are the coordinates of the
  trees.
\item
  The response is either \(pine\) (\(y=1\)) or \(redwood\) (\(y=-1\)).
\item
  \textbf{Goal}: to make a classifier for random seeds that we find on
  the ground for each of the three forests. The locations of the seeds
  are shown in the figure below (black circles).
\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  The point patterns of the known locations can be thought of as the
  training set.
\item
  The point pattern generated by the black circles can be thought of as
  the test set.
\end{itemize}

\vspace{4mm}

\begin{center}\includegraphics[width=0.9\linewidth]{9SVM_files/figure-beamer/trees2-1} \end{center}

\end{frame}

\begin{frame}{Maximal Margin Classifier}
\protect\hypertarget{maximal-margin-classifier}{}

\begin{block}{Hyperplane}

\vspace{2mm}

A \textbf{hyperplane} in \(p\) dimensions is defined as

\[\beta_0+\beta_1 X_1 + \beta_2 X_2 +...+\beta_p X_p=\beta_0+{\boldsymbol X}^T {\boldsymbol \beta}=0.\]
and is a \(p-1\) dimensional subspace of \(\mathbb{R}^p\).

\vspace{5mm}

\textbf{Recap}:

\begin{itemize}
\item
  If a point \({\boldsymbol x}=(x_1,x_2,...,x_p)^T\) satisfies the above
  equation, it lies on the hyperplane.
\item
  If \(\beta_0=0\) the hyperplane goes through the origin.
\item
  The vector \({\boldsymbol \beta}=(\beta_1, \ldots, \beta_p)\) (not
  including \(\beta_0\)) is called the normal vector and points in the
  direction orthogonal to the hyperplane.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

If a point \({\boldsymbol x}\) satisfies

\begin{itemize}
\item
  \(\beta_0+ {\boldsymbol \beta}^\top {\boldsymbol x}>0\) it lies on one
  side of the hyperplane
\item
  \(\beta_0+{\boldsymbol \beta}^\top {\boldsymbol x}<0\) it lies on the
  opposite side of the hyperplane.
\item
  \(\beta_0+{\boldsymbol \beta}^\top {\boldsymbol x}=0\) it lies on the
  hyperplane (by definition!).
\end{itemize}

\vspace{2mm}

\begin{itemize}
\item
  The signed distance \(d\) of any point \(\boldsymbol x\) to the
  hyperplane is given by
  \[d=\frac{1}{||{\boldsymbol\beta}||} (\beta_0 + {\boldsymbol \beta}^\top {\boldsymbol x} )\ ,\]
  where \(||{\boldsymbol\beta}||^2 = \sum_{j=1}^p \beta_j^2=1\) is the
  (squared) length of \(\boldsymbol \beta\) (Euclidian norm), see also
  \href{https://en.wikipedia.org/wiki/Distance_from_a_point_to_a_plane}{here}.
\item
  See board for a graphical description.
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{Assumptions}

\(~\)

\begin{itemize}
\tightlist
\item
  Assume that we have \(n\) training observations with \(p\) predictors
  \[{\boldsymbol x}_1=\left(
    \begin{array}{c}
      x_{11} \\
      \vdots \\
      x_{1p}
      \end{array}
  \right), \ldots, {\boldsymbol x}_n=\left(
    \begin{array}{c}
      x_{n1} \\
      \vdots \\
      x_{np}
      \end{array}
  \right) \ .\]
\end{itemize}

\(~\)

\begin{itemize}
\tightlist
\item
  The responses \(\boldsymbol{y}\) fall into two classes
  \(y_1,...,y_n \in \{-1,1\}\).
\end{itemize}

\(~\)

\begin{itemize}
\tightlist
\item
  It is possible to separate the training observations perfectly
  according to their class.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Possible hyperplanes (Forest 1)}

\vspace{2mm}

Which is ``best''?

\begin{center}\includegraphics[width=0.6\linewidth]{9SVM_files/figure-beamer/forest1.1-1} \end{center}

\vspace{2mm}

\textbf{Q:} How did we solve that problem earlier (e.g.~in LDA, QDA,
KNN,..)?

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Classification with a simple hyperplane}

\vspace{2mm}

All three hyperplanes have the property that

\[\beta_0+\beta_1 x_{i1} + \beta_2 x_{i2} =\beta_0+{\boldsymbol x}_i^T {\boldsymbol \beta}>0\]

if \(y_i=1\) (green points) and

\[\beta_0+\beta_1 x_{i1} + \beta_2 x_{i2} =\beta_0+{\boldsymbol x}_i^T {\boldsymbol \beta}<0\]
\vspace{2mm}

if \(y_i=-1\) (orange points).

\pause

\(~\)

In brief:

\[y_i (\beta_0+{\boldsymbol x}_i^T {\boldsymbol \beta})>0 \ .\]

\end{block}

\end{frame}

\begin{frame}

\vspace{2mm}

Analogous for \(p\) predictors: The class \(y^*\) of a new observation
\({\boldsymbol x}^*=(x_1^*,...,x_p^*)\) is assigned depending on the
value
\(f({\boldsymbol x}^*)=\beta_0+\beta_1 x_1^* + \beta_2 x_{2}^*+...+\beta_p x_{p}^*.\)

\vspace{2mm}

\textbf{Hyperplane classifier:}

\[y^* = \left\{ \begin{array}{ll}
1 \ , & \text{if } f({\boldsymbol x}^*) >0 \ ,\\
-1 \ , & \text{if } f({\boldsymbol x}^*) < 0 \ .
\end{array}\right.\]

\(~\)

Or

\[ y^*  f({\boldsymbol x}^*) > 0 \ . \]

\end{frame}

\begin{frame}

\begin{block}{Which hyperplane is best?}

\vspace{3mm}

\begin{itemize}
\item
  In general, if data are linearly separable, infinitely many possible
  separating hyperplanes exist.
\item
  Natural choice: the \textbf{maximal margin hyperplane}, which
  maximises the distance from the training observations.
\end{itemize}

\vspace{3mm}

\centering

\includegraphics[width=0.4\textwidth,height=\textheight]{../../ISLR/Figures/Chapter9/9.3.png}
\small

ISLR Figure 9.3

\end{block}

\end{frame}

\begin{frame}

The process of finding the maximal margin hyperplane for a dataset with
\(p\) covariates and \(n\) training observations can be formulated
through the following \textbf{optimization problem}:

\[\mathrm{maximize}_{\beta_0,\beta_1,...,\beta_p}  M \]
\[\text{subject to} \sum_{j=1}^p \beta_j^2=1,\]
\[y_i(\beta_0+\beta_1 x_{i1}+\beta_2 x_{i2}+...+\beta_p x_{ip})\geq M \quad  \forall i=1,...,n\]
where \(M\) is the width of the margin.

\(~\)

Observe: \vspace{-2mm}

\begin{itemize}
\tightlist
\item
  \(y_i(\beta_0+{\boldsymbol x}^T {\boldsymbol \beta})\) is the (signed)
  distance from the \(i\)th point to the hyperplane defined by the
  \(\beta\)s.
\item
  We want to find the hyperplane, where each observation is at least
  \(M\) units away - on the correct side, where \(M\) is as big as
  possible.
\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  In the ISLR Figure 9.3, the three equidistant point are called
  \textbf{support vectors}.
\item
  If one of the support vectors changes its position, the whole
  hyperplane will move.
\item
  This is a property of the maximal margin hyperplane: It only depends
  on the support vectors, and \emph{not on the other observations}.
\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}
\tightlist
\item
  It can be shown, see for example Hastie, Tibshirani, and Friedman
  (2009) Section 4.5.2 (\emph{Optimal Separating Hyperplanes}), that the
  optimization problem can be reformulated using Lagrange multipliers
  (primal and dual problem) into a quadratic convex optimization problem
  that can be solved
  efficiently\footnote{Since we in TMA4268 Statistical learning do not require a course in optimization - we do not go into details here.}.
\end{itemize}

\vspace{4mm}

\textbf{Q}: But why do we need to solve the optimization problem, if the
solution only depends on a few support vectors?

\textbf{A}: We do of course have to solve the optimization problem to
identify the support vectors and the unknown parameters for the
separating hyperplane.

\end{frame}

\begin{frame}

\begin{block}{Questions}

\vspace{2mm}

\begin{enumerate}
\item
  Explain briefly the idea behind the maximal margin classifier.
\item
  Is there any tuning parameters that needs to be chosen?
\item
  Look at the figure below. What could be the problem with the (maximal
  margin) hyperplane idea?
\end{enumerate}

\centering

\includegraphics[width=0.8\textwidth,height=\textheight]{../../ISLR/Figures/Chapter9/9.5.png}
\small

ISLR Figure 9.5

\end{block}

\end{frame}

\begin{frame}{Support Vector Classifiers}
\protect\hypertarget{support-vector-classifiers}{}

For some data sets a separating hyperplane does not exist, the data set
is \emph{non-separable}. What then? Forest 2:

\begin{center}\includegraphics[width=0.45\linewidth]{9SVM_files/figure-beamer/forest2.1-1} \end{center}

It is still possible to construct a hyperplane and use it for
classification, but then we have to \emph{allow some misclassification}
in the training data.

\end{frame}

\begin{frame}

\begin{itemize}
\item
  In some situations allowing for some misclassifications makes the
  class boundaries more robust to future observations (avoid
  overfitting).
\item
  Even when the data are linearly
  separable\footnote{in particular when $n\leq p+1$, we can always find a separating hyperplane, unless there are exact feature ties across the class barrier, see Efron and Hastie, 2016.},
  the separating hyperplane might not be the ``best'' hyperplane for us
  in terms of robustness.
\item
  We relax the maximal margin classifier to allow for a \emph{soft
  margin classifier} (=support vector classifier).
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{Optimization problem}

\vspace{2mm}

To obtain a \textbf{support vector classifier} we relax the conditions
that we had for the maximal margin hyperplane by allowing for a
``budget'' \(C\) of misclassifications: \vspace{2mm}

\[\mathrm{maximize}_{\beta_0,\beta_1,...,\beta_p}  M \]

\[\text{subject to} \sum_{j=1}^p \beta_j^2=1,\]
\[y_i(\beta_0+\beta_1 x_{i1}+\beta_2 x_{i2}+...+\beta_p x_{ip})\geq M(1-\epsilon_i) \quad  \forall i=1,...,n.\]
\[\epsilon_i\geq 0, \quad \sum_{i=1}^n \epsilon_i \leq C.\]

\end{block}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  \(M\) is the width of the margin.
\item
  \(\epsilon_1,...,\epsilon_n\) are
  \emph{\textcolor{red}{slack variables}}.

  \begin{itemize}
  \tightlist
  \item
    If \(\epsilon_i=0\) it means that observation \(i\) is on the
    correct side of the margin,
  \item
    if \(\epsilon_i>0\) observation \(i\) is on the wrong side of the
    margin, and
  \item
    if \(\epsilon_i>1\) observation \(i\) is on the wrong side of the
    hyperplane.
  \end{itemize}
\item
  \(C\) is a \emph{\textcolor{red}{tuning (regularization) parameter}}
  (chosen by cross-validation) giving the
  \emph{\textcolor{red}{budget for slacks}}. It restricts the number of
  the training observations that can be on the wrong side of the
  hyperplane. Less than \(C\) of the observations can be on the wrong
  side.
\end{itemize}

\end{frame}

\begin{frame}

Figure 19.3 in Efron and Hastie (2016) gives a nice graphical
representation of the soft margin classifier idea, where the violations
(estimates of \(\epsilon_i\)) are shown in green:

\centering

\includegraphics[width=0.8\textwidth,height=\textheight]{../../casi/19.3.png}

\end{frame}

\begin{frame}

\textbf{Classification rule:} We classify a test observation
\({\boldsymbol x}^*\) based on the sign of
\(f({\boldsymbol x}^*)=\beta_0+\beta_1 x_1^*+...+\beta_p x_p^*\) as
before:

\begin{itemize}
\tightlist
\item
  If \(f({\boldsymbol x}^*)<0\) then \(y^*=-1\).
\item
  If \(f({\boldsymbol x}^*)>0\) then \(y^*=1\).
\end{itemize}

\vspace{3mm}

More on solving the optimization problem: Hastie, Tibshirani, and
Friedman (2009) Section 12.2.1 (primal and dual Lagrange problem,
quadratic convex problem).

\end{frame}

\begin{frame}

\begin{itemize}
\item
  The hyperplane has the property that it \textbf{only} depends on the
  observations that \textbf{either lie on the margin or on the wrong
  side of the margin}. In fact, a noteworthy property of the solution is
  that \[\hat{\beta}= \sum_{i\in \mathcal{S}} \hat\alpha_i x_i \ ,\]
  where \(\mathcal{S}\) is a \emph{\textcolor{red}{support set}}.
\item
  The observations \(x_i, i \in \mathcal{S}\) are called our
  \textbf{support vectors}.
\item
  The observations on the correct side of the margin do not affect the
  support vectors. The length of distance for the support vectors to the
  class boundary is proportional to the slacks.
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{Questions}

\vspace{2mm}

\begin{enumerate}
\item
  Should the variables be standardized before used with this method?
\item
  The support vector classifier only depends on the observations that
  violate the margin. How does \(C\) affect the width of the margin?
\item
  Discuss how the tuning parameter \(C\) affects the bias-variance
  trade-off of the method.
\end{enumerate}

\end{block}

\end{frame}

\begin{frame}

\textbf{A:}

\end{frame}

\begin{frame}

\begin{block}{Role of the tuning parameter \(C\)}

ISLR Figure 9.7: From large \(C\) (top left) to small \(C\) (bottom
right). As C decreases the tolerance for observations being on the wrong
side of the margin decreases and the marin narrows.

\centering

\includegraphics[width=0.6\textwidth,height=\textheight]{../../ISLR/Figures/Chapter9/9.7.png}

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Example}

\vspace{2mm}

We will now find a support vector classifier for the second training
dataset (\texttt{forest2}) and use this to classify the observations in
the second test set (\texttt{seeds2}).

\begin{itemize}
\tightlist
\item
  There are \(100\) observations of trees: 45 pines (\(y_i=1\)) and 55
  redwood trees (\(y_i=-1\)).
\item
  In the test set there are 20 seeds: 10 pine seeds and 10 redwood
  seeds.
\end{itemize}

\vspace{3mm}

The function \texttt{svm} in the package \texttt{e1071} is used to find
the maximal margin hyperplane. The response needs to be coded as a
factor variable, and the data set has to be stored as a dataframe.

\vspace{2mm}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(e1071)}
\NormalTok{forest2 =}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\DataTypeTok{file =} \StringTok{"forest2.txt"}\NormalTok{)}
\NormalTok{seeds2 =}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\DataTypeTok{file =} \StringTok{"seeds2.txt"}\NormalTok{)}
\NormalTok{train2 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ forest2[, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{], }\DataTypeTok{y =} \KeywordTok{as.factor}\NormalTok{(forest2[, }\DecValTok{3}\NormalTok{]))}
\NormalTok{test2 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ seeds2[, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{], }\DataTypeTok{y =} \KeywordTok{as.factor}\NormalTok{(seeds2[, }\DecValTok{3}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\normalsize

\end{block}

\end{frame}

\begin{frame}[fragile]

The \texttt{svm} function uses a slightly different formulation than
what we introduced above.

\begin{itemize}
\tightlist
\item
  We a \emph{budget} of errors \(C\), but in \texttt{svm} we instead
  have an argument \texttt{cost} that allows us to specify the cost of
  violating the margin. You can think of the cost as
  \(\propto \frac{1}{C}\).
\end{itemize}

\vspace{3mm}

We first try with \texttt{cost=1}. We set
\texttt{kernel=\textquotesingle{}linear\textquotesingle{}} as we are
interested in a linear decision boundary. \texttt{scale=TRUE} scales the
predictors to have mean 0 and standard deviation 1. We choose
\texttt{scale=FALSE} (no scaling).

\(~\)

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svmfit_linear1 =}\StringTok{ }\KeywordTok{svm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ train2, }\DataTypeTok{kernel =} \StringTok{"linear"}\NormalTok{, }\DataTypeTok{cost =} \DecValTok{1}\NormalTok{, }
    \DataTypeTok{scale =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{frame}

\begin{frame}[fragile]

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(svmfit_linear1, train2, }\DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"lightcoral"}\NormalTok{, }\StringTok{"lightgreen"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.8\linewidth]{9SVM_files/figure-beamer/forest2.2-1} \end{center}

(Note: The decision boundary looks a bit strange, and \(x_1\) is plotted
on the (usual) \(y\)-axis and \(x_2\) on the \(x\)-axis. Both problems
are due to implementation, nothing for us to worry.)

\end{frame}

\begin{frame}[fragile]

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(svmfit_linear1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## svm(formula = y ~ ., data = train2, kernel = "linear", cost = 1, 
##     scale = FALSE)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  1 
## 
## Number of Support Vectors:  56
## 
##  ( 28 28 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  -1 1
\end{verbatim}

\normalsize

\end{frame}

\begin{frame}[fragile]

\textbf{Observations}

\begin{itemize}
\tightlist
\item
  The crosses in the plot indicate the support vectors, whose index can
  be obtained from
\end{itemize}

\vspace{2mm}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svmfit_linear1}\OperatorTok{$}\NormalTok{index  }\CommentTok{#support vectors id in data set}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  1  2  4  6  9 10 16 21 26 27 28 40 44 53 55 57 58 65 67 72 76 77 80 81 87
## [26] 91 92 98  5  8 11 13 18 19 20 23 24 25 34 36 39 41 42 47 48 59 61 62 70 71
## [51] 75 78 88 93 95 96
\end{verbatim}

\normalsize

\(~\)

\begin{itemize}
\item
  With \texttt{cost=1}, we have 56 support vectors, 28 in each class.
\item
  All other data points are shown as circles.
\item
  However, no explicit output for the linear decision boundary, and no
  margin width is given by \texttt{svm()}. Want to see how to find this?
  See the recommended exercises.
\end{itemize}

\end{frame}

\begin{frame}[fragile]

Next, we set \texttt{cost=100}:

\vspace{2mm}

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svmfit_linear2 =}\StringTok{ }\KeywordTok{svm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ train2, }\DataTypeTok{kernel =} \StringTok{"linear"}\NormalTok{, }\DataTypeTok{cost =} \DecValTok{100}\NormalTok{, }
    \DataTypeTok{scale =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(svmfit_linear2, train2, }\DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"lightcoral"}\NormalTok{, }\StringTok{"lightgreen"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{9SVM_files/figure-beamer/forest2.3-1} \end{center}

\end{frame}

\begin{frame}[fragile]

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(svmfit_linear2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## svm(formula = y ~ ., data = train2, kernel = "linear", cost = 100, 
##     scale = FALSE)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  100 
## 
## Number of Support Vectors:  31
## 
##  ( 15 16 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  -1 1
\end{verbatim}

\normalsize

Thus with \texttt{cost=100} we have 31 support vectors. The width of the
margin is
decreased.\footnote{Remember: higher cost = lower budget to violate the boundaries}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Cross-validation to find an optimal \texttt{cost}}

\vspace{2mm}

The \texttt{cost} is a tuning parameter. By using the \texttt{tune()}
function we can perform 10-fold cross-validation and find the
cost-parameter that gives the lowest cross-validation error:

\vspace{2mm}

\tiny

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{CV_linear =}\StringTok{ }\KeywordTok{tune}\NormalTok{(svm, y }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ train2, }\DataTypeTok{kernel =} \StringTok{"linear"}\NormalTok{, }\DataTypeTok{ranges =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{cost =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.001}\NormalTok{, }
    \FloatTok{0.01}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{50}\NormalTok{)))}
\KeywordTok{summary}\NormalTok{(CV_linear)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Parameter tuning of 'svm':
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  cost
##     5
## 
## - best performance: 0.14 
## 
## - Detailed performance results:
##    cost error dispersion
## 1 1e-03  0.45 0.10801234
## 2 1e-02  0.23 0.12516656
## 3 1e-01  0.16 0.11737878
## 4 1e+00  0.15 0.10801234
## 5 5e+00  0.14 0.10749677
## 6 1e+01  0.15 0.09718253
## 7 5e+01  0.15 0.09718253
\end{verbatim}

\end{block}

\end{frame}

\begin{frame}[fragile]

According to the \texttt{tune()} function we should set the cost
parameter to 5. The function also stores the best model obtained and we
can access it as follows:

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bestmod_linear =}\StringTok{ }\NormalTok{CV_linear}\OperatorTok{$}\NormalTok{best.model}
\end{Highlighting}
\end{Shaded}

\normalsize

Next, we want to predict the class label of the seeds in the test set.
We use the \texttt{predict} function and make a confusion table:

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ypred_linear =}\StringTok{ }\KeywordTok{predict}\NormalTok{(bestmod_linear, test2)}
\KeywordTok{table}\NormalTok{(}\DataTypeTok{predict =}\NormalTok{ ypred_linear, }\DataTypeTok{truth =}\NormalTok{ test2[, }\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        truth
## predict -1  1
##      -1  8  0
##      1   2 10
\end{verbatim}

\normalsize

Thus two of the seeds are misclassified, the other 18 are ok.

\end{frame}

\begin{frame}

\footnotesize

\begin{center}\includegraphics[width=0.9\linewidth]{9SVM_files/figure-beamer/forest2.4-1} \end{center}

\normalsize

The two misclassified observations are marked with a black circle.

\end{frame}

\begin{frame}{Support Vector Machines}
\protect\hypertarget{support-vector-machines-1}{}

\begin{itemize}
\item
  For some datasets a
  \emph{\textcolor{red}{non-linear decicion boundary}} between the
  classes is more suitable than a linear decision boundary.
\item
  In such cases you can use a \textbf{Support Vector Machine} (SVM).
  This is an extension of the support vector classifier.
\end{itemize}

\begin{center}\includegraphics[width=0.4\linewidth]{9SVM_files/figure-beamer/forest3.1-1} \end{center}

\end{frame}

\begin{frame}

\begin{block}{Expanding the feature space}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Recall from Module 7: We could fit non-linear regression curves by
  using a polynomial basis. This was a \textbf{linear regression in the
  transformed variables}, but non-linear in the original variables.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  \textbf{Idea}: Find a linear boundary in that high-dimensional space
  using

  \begin{itemize}
  \tightlist
  \item
    higher-order terms \(X_i^k\)
  \item
    interaction terms \(X_i X_{i'}\)
  \item
    other functions of \(X_i\).
  \end{itemize}
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  This leads to a non-linear boundary in the original space.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  \textbf{Example}:
  \[\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1X_2 + \beta_4 X_1^2 + \beta_5 X_2^2 =0 \ .\]
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Example}

(ISRL Figure 9.9)

\centering

\includegraphics[width=0.8\textwidth,height=\textheight]{../../ISLR/Figures/Chapter9/9.9.png}

\flushleft

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  \textbf{Left}: expanding feature space to include cubic polynomials
  (\(x_1,x_2,x_1x_2,x_1^2,x_2^2,x_1^2x_2,x_1x_2^2x_1^3,x_2^3\), 9
  parameters to estimate in addition to intercept), and also observe the
  margins (interpretation?).
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  \textbf{Right}: radial basis function kernel - wait a bit.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Problems and better ideas}

\(~\)

\begin{itemize}
\tightlist
\item
  Computation using polynomials quickly becomes unmanageable.
\end{itemize}

\(~\)

\begin{itemize}
\tightlist
\item
  Higher-order polynomials become very wiggly and wild.
\end{itemize}

\(~\)

\begin{itemize}
\tightlist
\item
  More elegant idea is the use of \emph{\textcolor{red}{kernels}}.
\end{itemize}

\(~\)

\begin{itemize}
\tightlist
\item
  But first we have to understand the role of
  \emph{\textcolor{red}{inner products}} in Support Vector Classifiers.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{block}{Inner products}

\vspace{2mm}

The \emph{\textcolor{red}{inner product}} between two observations \(i\)
and \(i'\) is defined as \[
\langle {\boldsymbol x}_i , {\boldsymbol x}_{i'}\rangle =\sum_{j=1}^p x_{ij} x_{i' j} \ .
\] The inner product encodes for the \emph{\textcolor{red}{similarity}}
between observations.

\end{block}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  Remember the optimisation problem of finding the support vector
  classifier hyperplane. We have \emph{not} explained how to solve the
  problem because this is outside the scope of this course. \vspace{1mm}
\item
  But we said that
  \[\hat{\beta}= \sum_{i\in \mathcal{S}} \hat\alpha_i x_i \] for a
  support set \(\mathcal{S}\). \vspace{1mm}
\end{itemize}

\begin{itemize}
\tightlist
\item
  Thus the solution function \(\hat{f}\) to the support vector
  classifier problem at a new observation \({\boldsymbol x}\), can be
  expressed as \begin{align*}
  \hat{f}(x) & = \hat{\beta}_0 + x^\top \hat\beta  \\
   & = \hat\beta_0 + \sum_{i \in \mathcal{S}} \hat\alpha_i \langle x,x_i \rangle \ ,
  \end{align*} where \(\alpha_i\) is some parameter and \(i=1,...,n\).
\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  This implies that to estimate the parameters
  \(\beta_0,\alpha_1,...,\alpha_n\) we only need to know the
  \({n \choose 2}\) inner products
  \(\langle {\boldsymbol x}_i,{\boldsymbol x}_i' \rangle\) between all
  pair of training observations (and their correct
  class)\footnote{For the interested reader: See Section 12.2.1 in James et al (2013), or Eq. 19.22 and 19.23 of Efron and Hastie (2016).}.
\item
  Even better, \(\alpha_i \neq 0\) for the support vectors \(x_i\)
  (\(i \in \mathcal{S}\)), while \(\alpha_i=0\) for all the rest.
\end{itemize}

\(~\)

This will bring us to another idea:

\begin{itemize}
\tightlist
\item
  All we need is to know to classify a new observations is the
  \textbf{similarity} to all the training observations, where similarity
  could be defined in any way.
\end{itemize}

\end{frame}

\begin{frame}

\textbf{Q:} Find the support vectors

\includegraphics{../../ISLR/Figures/Chapter9/9.6.png}

\begin{itemize}
\tightlist
\item
  \textbf{Q}: So why don't we just directly only use the support
  vectors?
\item
  \textbf{A}: 
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{Kernels}

\(~\)

\begin{itemize}
\tightlist
\item
  Idea: replace the innerproduct
  \(\langle {\boldsymbol x}_i, {\boldsymbol x}_{i'}\rangle\) as measure
  of similarity by a more general concept: Replace
  \(\langle {\boldsymbol x}_i, {\boldsymbol x}_{i'}\rangle\) by a
  \emph{\textcolor{red}{kernel}}
  \(K({\boldsymbol x}_i,{\boldsymbol x}_{i'})\), such that \[
  f({\boldsymbol x})=\beta_0 + \sum_{i \in \mathcal{S}} \alpha_i K({\boldsymbol x},{\boldsymbol x}_i).
  \]
\end{itemize}

\(~\)

\begin{itemize}
\tightlist
\item
  For the familiar linear case, the kernel is simply the inner product
  (\emph{linear kernel})
  \(K({\boldsymbol x}_i,{\boldsymbol x}_i')=\sum_{j=1}^p x_{ij}x_{i'j}\).
\end{itemize}

\begin{itemize}
\tightlist
\item
  If we want a more flexible decision boundary we could instead use a
  \emph{polynomial kernel} of degree \(d>1\): \[
  K({\boldsymbol x}_i,{\boldsymbol x}_i')=(1+\sum_{j}^p x_{ij} x_{i'j})^d \ , 
  \] which computes the inner products needed for \(d\)-dimensional
  polynomials -- without really visiting the whole space. Try it for
  \(p=2\) and \(d=2\) (see compulsory exercise 2).
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{itemize}
\item
  By using \emph{non-linear kernels}, the resulting classifier is a
  \emph{\textcolor{red}{support vector machine}}.
\item
  The nice thing here is that we only need to calculate the kernels,
  \emph{not the basis functions}.
\item
  We only need to compute \(K(x_i,x_{i'})\) for the \({n \choose 2}\)
  distinct pairs -- without explicitly working in an enlarged feature
  space. This is very useful when there are \emph{many features}, i.e.,
  \(p\geq n\).\footnote{And this is the reason why $p>n$ is possible in support vector machines.}
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{The radial kernel}

\(~\)

\begin{itemize}
\tightlist
\item
  A very popular choice is the \emph{radial kernel}, \[
  K({\boldsymbol x}_i,{\boldsymbol x}_i')=\exp(-\gamma \sum_{j=1}^p (x_{ij}-x_{i'j})^2) \ ,
  \] where \(\gamma\) is a positive constant (a tuning parameter).
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Interestingly, the radial kernel computes the inner product in a
  infinite dimensional feature space. But, this does not give
  overfitting because some of the dimensions are ``squashed down''
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  We have the parameter \(\gamma\) and the budget parameter \(C\) to
  decide on.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Connection to a multivariate normal density, where
  \(\gamma \propto 1/\sigma^2\) (\(\sigma^2\) variance in normal
  distribution). If \(\gamma\) is small (similar to large variance in
  the normal distribution) the decision boundaries are smoother than for
  larger \(\gamma\).
\end{itemize}

\end{block}

\end{frame}

\begin{frame}

\begin{itemize}
\tightlist
\item
  The radial kernel is convenient if we want a circular decision
  boundary. See Figure 19.5 in Efron and Hastie (2016): \vspace{2mm}
\end{itemize}

\includegraphics{casi19.5.png}

\begin{itemize}
\item
  \(\gamma\) and our budget \(C\) can be chosen by cross-validation.
\item
  Remark: the mathematics behind this is based on
  \emph{reproducing-kernel Hilbert spaces} (see page 384 of Efron and
  Hastie (2016) for a glimpse of the theory).
\end{itemize}

\end{frame}

\begin{frame}

Study Figures 19.5 and 19.6 (page 383) in Efron and Hastie (2016) to see
how the radial kernel can make smooth functions.

\centering

\includegraphics[width=0.7\textwidth,height=\textheight]{../../casi/19.6.png}

\flushleft
\small

If you want to download the whole book you can do this here:

\href{https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf}{Computer
Age Statistical Inference}

\end{frame}

\begin{frame}

\begin{block}{Kernels and our optimization}

\vspace{2mm}

We now merge our optimization problem (from our support vector
classifier) with our kernel representation \(f({\boldsymbol x})\) to get
the

\vspace{4mm}

\textbf{Support Vector Machine (SVM)}

\[\mathrm{maximize}_{\beta_0,\alpha_1,...,\alpha_n,\epsilon_1,...,\epsilon_n,} \quad M \]
\[y_i \cdot f({\boldsymbol x}_i)\geq M(1-\epsilon_i) \quad  \forall i=1,...,n \ ,\]
\[\epsilon_i\geq 0, \quad \sum_{i=1}^n \epsilon_i \leq C \ ,\]

where
\[f({\boldsymbol x}_i)=\beta_0 +\sum_{l=1}^n \alpha_l K({\boldsymbol x}_i,{\boldsymbol x}_l) \ .\]

\end{block}

\end{frame}

\begin{frame}[fragile]

\begin{block}{Example: forest 3}

\vspace{2mm}

To illustrate the SVM we use the third training dataset
(\texttt{forest3}) and the third test set (\texttt{seeds3}). We use the
\texttt{svm} function as before. However, we now set
\texttt{kernel=\textquotesingle{}radial\textquotesingle{}} as we want a
non-linear decision boundary:

\(~\)

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(e1071)}
\NormalTok{forest3 =}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\DataTypeTok{file =} \StringTok{"forest3.txt"}\NormalTok{)}
\NormalTok{seeds3 =}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\DataTypeTok{file =} \StringTok{"seeds3.txt"}\NormalTok{)}
\NormalTok{train3 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ forest3[, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{], }\DataTypeTok{y =} \KeywordTok{as.factor}\NormalTok{(forest3[, }\DecValTok{3}\NormalTok{]))}
\NormalTok{test3 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ seeds3[, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{], }\DataTypeTok{y =} \KeywordTok{as.factor}\NormalTok{(seeds3[, }\DecValTok{3}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\end{block}

\end{frame}

\begin{frame}[fragile]

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svmfit_kernel1 =}\StringTok{ }\KeywordTok{svm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ train3, }\DataTypeTok{kernel =} \StringTok{"radial"}\NormalTok{, }\DataTypeTok{cost =} \DecValTok{10}\NormalTok{, }
    \DataTypeTok{scale =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(svmfit_kernel1, train3, }\DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"lightcoral"}\NormalTok{, }\StringTok{"lightgreen"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{9SVM_files/figure-beamer/forest3.3-1} \end{center}

\end{frame}

\begin{frame}[fragile]

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(svmfit_kernel1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## svm(formula = y ~ ., data = train3, kernel = "radial", cost = 10, 
##     scale = FALSE)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  radial 
##        cost:  10 
## 
## Number of Support Vectors:  42
## 
##  ( 22 20 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  -1 1
\end{verbatim}

\normalsize

\end{frame}

\begin{frame}[fragile]

We could also try with a polynomial kernel with degree 4 as follows:

\(~\)

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svmfit_kernel2 =}\StringTok{ }\KeywordTok{svm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ train3, }\DataTypeTok{kernel =} \StringTok{"polynomial"}\NormalTok{, }\DataTypeTok{degree =} \DecValTok{4}\NormalTok{, }
    \DataTypeTok{cost =} \DecValTok{10000}\NormalTok{, }\DataTypeTok{scale =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(svmfit_kernel2, train3, }\DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"lightcoral"}\NormalTok{, }\StringTok{"lightgreen"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{9SVM_files/figure-beamer/forest3.4-1} \end{center}

\end{frame}

\begin{frame}[fragile]

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(svmfit_kernel2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## svm(formula = y ~ ., data = train3, kernel = "polynomial", degree = 4, 
##     cost = 10000, scale = FALSE)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  polynomial 
##        cost:  10000 
##      degree:  4 
##      coef.0:  0 
## 
## Number of Support Vectors:  41
## 
##  ( 21 20 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  -1 1
\end{verbatim}

\end{frame}

\begin{frame}[fragile]

For this dataset a radial kernel is a natural choice: A circular
decision boundary seems like a good idea. Thus, we proceed with
\texttt{kernel=\textquotesingle{}radial\textquotesingle{}}, and use the
\texttt{tune()} function to find (approximately) optimal tuning
parameters \(C\) and \(\gamma\):

\(~\)

\tiny

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{CV_kernel =}\StringTok{ }\KeywordTok{tune}\NormalTok{(svm, y }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ train3, }\DataTypeTok{kernel =} \StringTok{"radial"}\NormalTok{, }\DataTypeTok{ranges =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{cost =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }
    \FloatTok{0.1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{1000}\NormalTok{), }\DataTypeTok{gamma =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{)))}
\KeywordTok{summary}\NormalTok{(CV_kernel)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Parameter tuning of 'svm':
## 
## - sampling method: 10-fold cross validation 
## 
## - best parameters:
##  cost gamma
##     5   0.1
## 
## - best performance: 0.09642857 
## 
## - Detailed performance results:
##     cost gamma      error dispersion
## 1  1e-02 1e-02 0.27321429 0.14726580
## 2  1e-01 1e-02 0.27321429 0.14726580
## 3  1e+00 1e-02 0.27321429 0.14726580
## 4  5e+00 1e-02 0.27321429 0.14726580
## 5  1e+01 1e-02 0.27321429 0.14726580
## 6  1e+02 1e-02 0.27321429 0.14726580
## 7  1e+03 1e-02 0.11250000 0.11232507
## 8  1e-02 1e-01 0.27321429 0.14726580
## 9  1e-01 1e-01 0.27321429 0.14726580
## 10 1e+00 1e-01 0.27321429 0.14726580
## 11 5e+00 1e-01 0.09642857 0.09494001
## 12 1e+01 1e-01 0.11250000 0.11232507
## 13 1e+02 1e-01 0.09642857 0.09494001
## 14 1e+03 1e-01 0.12142857 0.11566626
## 15 1e-02 1e+00 0.27321429 0.14726580
## 16 1e-01 1e+00 0.27321429 0.14726580
## 17 1e+00 1e+00 0.15000000 0.13154627
## 18 5e+00 1e+00 0.13571429 0.12262483
## 19 1e+01 1e+00 0.12142857 0.12981104
## 20 1e+02 1e+00 0.10892857 0.10662906
## 21 1e+03 1e+00 0.17678571 0.12269704
## 22 1e-02 1e+01 0.27321429 0.14726580
## 23 1e-01 1e+01 0.27321429 0.14726580
## 24 1e+00 1e+01 0.18928571 0.13261926
## 25 5e+00 1e+01 0.15892857 0.17666643
## 26 1e+01 1e+01 0.15892857 0.17666643
## 27 1e+02 1e+01 0.14642857 0.16683665
## 28 1e+03 1e+01 0.14642857 0.16683665
## 29 1e-02 1e+02 0.27321429 0.14726580
## 30 1e-01 1e+02 0.27321429 0.14726580
## 31 1e+00 1e+02 0.23035714 0.14395658
## 32 5e+00 1e+02 0.23035714 0.15892969
## 33 1e+01 1e+02 0.23035714 0.15892969
## 34 1e+02 1e+02 0.23035714 0.15892969
## 35 1e+03 1e+02 0.23035714 0.15892969
\end{verbatim}

\normalsize

\end{frame}

\begin{frame}[fragile]

The optimal \(C\) is 100. Next, we predict the class label of the seeds
in the test set with a model with \(C=100\), make a confusion table and
plot the results:

\(~\)

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bestmod_kernel =}\StringTok{ }\NormalTok{CV_kernel}\OperatorTok{$}\NormalTok{best.model}
\NormalTok{ypred_kernel =}\StringTok{ }\KeywordTok{predict}\NormalTok{(bestmod_kernel, test3)}
\end{Highlighting}
\end{Shaded}

\(~\)

\begin{center}\includegraphics[width=0.9\linewidth]{9SVM_files/figure-beamer/forest3.5-1} \end{center}

\end{frame}

\begin{frame}[fragile]

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(}\DataTypeTok{predict =}\NormalTok{ ypred_kernel, }\DataTypeTok{truth =}\NormalTok{ test3[, }\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        truth
## predict -1 1
##      -1  9 0
##      1   1 5
\end{verbatim}

\normalsize

Only one seed is misclassified.

\end{frame}

\begin{frame}

\begin{block}{Tuning parameter example}

\vspace{2mm}

Heart data - predict heart disease from \(p=13\) predictors.

Training errors as ROC and AUC.

\vspace{1mm}

\(~\) \centering

\includegraphics[width=0.8\textwidth,height=\textheight]{../../ISLR/Figures/Chapter9/9.10.png}

\(~\)

\flushleft

\textbf{Q}: How are the ROC curves formed?

\end{block}

\end{frame}

\begin{frame}

Heart data - test error.

\centering

\includegraphics[width=0.8\textwidth,height=\textheight]{../../ISLR/Figures/Chapter9/9.11.png}

\end{frame}

\begin{frame}[fragile]{Extensions}
\protect\hypertarget{extensions}{}

\begin{block}{More than two classes}

\vspace{2mm}

What if we have \(k\) classes? Two popular approaches:

\(~\)

\begin{itemize}
\tightlist
\item
  OVA: \emph{\textcolor{red}{one-versus-all}}. Fit \(k\) different
  two-class SVMs \(f_k({\boldsymbol x})\) where, each time, one class is
  compared to the ensemble of all other classes. Classify a test
  observation to the class where \(f_k({\boldsymbol x}^*)\) is largest.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  OVO: \emph{\textcolor{red}{one-versus-one}}. \texttt{libsvm} uses this
  approach, in which \(k(k-1)/2\) binary classifiers are trained; the
  appropriate class is found by a voting scheme (the class that wins the
  most pairwise competitions for a given \({\boldsymbol x}^*\) is
  chosen).
\end{itemize}

\end{block}

\end{frame}

\begin{frame}{Comparisons: SVM and logistic regression}
\protect\hypertarget{comparisons-svm-and-logistic-regression}{}

It is possible to write the optimization problem for the support vector
classifier as a ``loss''+``penalty'':

\[\text{minimize}_{\boldsymbol \beta} \left\{ \underbrace{\sum_{i=1}^n \max(0,1-y_i f({\boldsymbol x}_i))}_{=L(\boldsymbol x, \boldsymbol{y},\boldsymbol\beta)}+ \underbrace{\lambda \sum_{j=1}^p \beta_j^2}_{\text{Penalty}} \right\}\]

\begin{itemize}
\item
  The margin now corresponds to \(M=1\), and its width is determined by
  \(\sum_{j=1}^p \beta_j^2\).
\item
  The loss \(L(\boldsymbol x, \boldsymbol{y},\boldsymbol\beta)\) is
  called \emph{\textcolor{red}{hinge loss}} - observe the max and 0 to
  explain why only support vectors contribute.
\item
  The penalty is a ridge penalty.
\item
  Large \(\lambda\) gives small \(\beta\)s and more violations=high
  bias, but low variance -- and vice versa.
\end{itemize}

\end{frame}

\begin{frame}

Interestingly, the loss functions for a support-vector classifier with
\(f(X)=\beta_0 + \beta_1X_1 +\ldots + \beta_pX_p\) and for logistic
regression using the same set of predictors \emph{look very similar}:
\vspace{-2mm}

\centering

\includegraphics[width=0.6\textwidth,height=\textheight]{../../ISLR/Figures/Chapter9/9.12.png}

\end{frame}

\begin{frame}

\textbf{Hinge loss:} \[\max(0,1-y_if({\boldsymbol x}_i))\]

\begin{itemize}
\item
  For comparison a logistic regression loss function would be (binomial
  deviance with \(-1,1\) coding of \(y\))
  \[ \log(1+\exp(-y_i f({\boldsymbol x}_i)))\ .\]
\item
  In logistic regression all observations contribute weighted by
  \(p_i(1-p_i)\) (where \(p_i\) is probability for class 1), that fade
  smoothly with distance to the decision boundary
\item
  Of course, it is possible to extend the logistic regression to include
  non-linear terms, and \emph{also a ridge penalty}.
\end{itemize}

\end{frame}

\begin{frame}

\begin{block}{When to use SVM?}

\(~\)

\begin{itemize}
\tightlist
\item
  If classes are nearly separable SVM will perform better than logistic
  regression. (Also LDA will perform better than logistic regression;
  what is the problem with logistic regression?)
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Otherwise, a ridge penalty version of logistic regression is
  \textbf{very} similar to SVM, and logistic regression will also give
  you probabilities for each class.
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  If class boundaries are non-linear then SVM is more popular, but
  \emph{kernel versions of logistic regression} are also possible, but
  more computationally expensive (and traditionally less used).
\end{itemize}

\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Drawbacks of SVMs: No feature selection, no probabilities, thus in
  general interpretability is difficult.
\end{itemize}

\end{block}

\end{frame}

\begin{frame}{Summing up}
\protect\hypertarget{summing-up}{}

\begin{itemize}
\item
  We use methods from computer science, not probability models, but it
  also looks for a separating hyperplane in (an extended) feature space
  in the classification setting.
\item
  SVM is a widely successful and a ``must have tool''.
\item
  Interpretation of SVM: all features are included and maybe not so easy
  to interpret (remember ridge-type penalty does not shrink to zero).
\item
  The budget must be chosen wisely, and a bad choice can lead to
  overfitting.
\item
  Not so easy to get class probabilites from SVM (what is done is
  actually to fit a logistic regression after fitting SVM).
\end{itemize}

\end{frame}

\begin{frame}{Further reading}
\protect\hypertarget{further-reading}{}

\begin{itemize}
\item
  \href{https://www.youtube.com/playlist?list=PL5-da3qGB5IDl6MkmovVdZwyYOhpCxo5o}{Videoes
  on YouTube by the authors of ISL, Chapter 9}, and corresponding
  \href{https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/svm.pdf}{slides}
\item
  \href{https://rpubs.com/ppaquay/65566}{Solutions to exercises in the
  book, chapter 9}
\item
  \href{https://web.stanford.edu/~hastie/ElemStatLearn}{Chapters
  12.1-12.3} in Hastie, Tibshirani, and Friedman (2009).
\item
  \href{https://web.stanford.edu/~hastie/CASI/}{Chapter 19
  (Support-Vector Machines and Kernel Methods)} in Efron and Hastie
  (2016).
\end{itemize}

\end{frame}

\begin{frame}{References}
\protect\hypertarget{references}{}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-casi}{}%
Efron, Bradley, and Trevor Hastie. 2016. \emph{Computer Age Statistical
Inference - Algorithms, Evidence, and Data Science}. Cambridge
University Press.

\leavevmode\hypertarget{ref-ESL}{}%
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. \emph{The
Elements of Statistical Learning}. 2nd ed. Vol. 1. Springer series in
statistics New York.

\leavevmode\hypertarget{ref-james.etal}{}%
James, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. \emph{An
Introduction to Statistical Learning with Applications in R}. New York:
Springer.

\end{frame}

\end{document}
